# GPT Local

![Estado](https://img.shields.io/badge/estado-beta-green)
![Python](https://img.shields.io/badge/python-3.8+-blue)
![License](https://img.shields.io/badge/license-MIT-brightgreen)
![Framework](https://img.shields.io/badge/framework-GPT4All-orange)

Sistema de chat local que utiliza la API de GPT4All para interactuar con modelos de lenguaje de manera eficiente y flexible.

## ğŸŒŸ CaracterÃ­sticas

- ğŸ’¬ Chat interactivo con modelos locales
- ğŸ”„ Soporte para mÃºltiples modelos de GPT4All
- ğŸ“Š Sistema de mÃ©tricas y estadÃ­sticas de uso
- ğŸ’¾ Persistencia de conversaciones
- ğŸŒ Interfaz web moderna (Vue.js)
- ğŸ¨ Modo oscuro/claro
- âš™ï¸ ConfiguraciÃ³n flexible de parÃ¡metros

## ğŸš€ Inicio RÃ¡pido

### Requisitos Previos

- Python 3.8+
- Node.js 16+
- GPT4All instalado localmente

### InstalaciÃ³n

1. Clonar el repositorio:
```bash
git clone https://github.com/JoseLVillaronga/GPT_Local.git
cd GPT_Local
```

2. Instalar dependencias de Python:
```bash
pip install -r requirements.txt
```

3. Instalar dependencias del frontend:
```bash
cd gpt_local_web/frontend
npm install
```

### EjecuciÃ³n

1. Iniciar el chat en consola:
```bash
python test_local_llms.py
```

2. Iniciar la aplicaciÃ³n web:
```bash
# Backend
cd gpt_local_web
python app.py

# Frontend (en otra terminal)
cd gpt_local_web/frontend
npm run dev
```

## ğŸ“š Ejemplos de Uso

### Chat en Consola

```python
# Ejemplo de sesiÃ³n de chat bÃ¡sica
python test_local_llms.py

# Con configuraciÃ³n especÃ­fica
python test_local_llms.py --model "orca-mini-3b" --temperature 0.7

# Exportar conversaciÃ³n
python test_local_llms.py --export markdown
```

### Configuraciones Comunes

```python
# ConfiguraciÃ³n para respuestas creativas
{
    'temperature': 0.9,
    'max_tokens': 300,
    'top_p': 0.95
}

# ConfiguraciÃ³n para respuestas precisas
{
    'temperature': 0.3,
    'max_tokens': 150,
    'top_p': 0.8
}
```

### API Web

```javascript
// Ejemplo de llamada a la API
const response = await fetch('http://localhost:8000/api/chat', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json'
    },
    body: JSON.stringify({
        message: "Â¿CÃ³mo funciona GPT4All?",
        model: "orca-mini-3b",
        config: {
            temperature: 0.7
        }
    })
});
```

## ğŸ”§ Troubleshooting

### Requerimientos de Sistema
- **RAM**: MÃ­nimo 8GB, Recomendado 16GB+
- **CPU**: Procesador moderno con soporte AVX2
- **Almacenamiento**: 10GB+ para modelos y datos

### Problemas Comunes

1. **Error: Model not found**
   ```
   SoluciÃ³n: Verificar que el modelo estÃ¡ descargado en la carpeta correcta
   Por defecto: ~/.local/share/nomic.ai/GPT4All/
   ```

2. **Error: CUDA not available**
   ```
   SoluciÃ³n: La aceleraciÃ³n GPU es opcional. El sistema funciona con CPU.
   Para habilitar GPU, instalar CUDA toolkit y pytorch con soporte CUDA.
   ```

3. **Error: API connection refused**
   ```
   SoluciÃ³n: Verificar que el backend estÃ¡ corriendo en puerto 8000
   Check: netstat -tulpn | grep 8000
   ```

## ğŸ—ºï¸ Roadmap

### Fase Actual (Beta)
- [x] ImplementaciÃ³n bÃ¡sica del chat
- [x] Interfaz web funcional
- [x] Sistema de persistencia
- [x] MÃ©tricas bÃ¡sicas

### PrÃ³ximas CaracterÃ­sticas
- [ ] Soporte para mÃºltiples sesiones simultÃ¡neas
- [ ] Mejoras en la interfaz web
- [ ] Sistema de cachÃ© para respuestas frecuentes
- [ ] IntegraciÃ³n con mÃ¡s modelos

### InvestigaciÃ³n Futura
- [ ] CooperaciÃ³n entre diferentes modelos
- [ ] OptimizaciÃ³n de parÃ¡metros automÃ¡tica
- [ ] Sistema de fine-tuning local

## ğŸ› ï¸ Estructura del Proyecto

```
GPT_Local/
â”œâ”€â”€ test_local_llms.py      # Cliente de chat en consola
â”œâ”€â”€ requirements.txt        # Dependencias Python
â”œâ”€â”€ docs/                  # DocumentaciÃ³n
â”œâ”€â”€ chat_history/         # Historial de conversaciones
â”œâ”€â”€ chat_exports/        # Exportaciones de chat
â””â”€â”€ gpt_local_web/      # AplicaciÃ³n web
    â”œâ”€â”€ frontend/      # Frontend Vue.js
    â””â”€â”€ backend/      # API REST
```

## ğŸ“ Funcionalidades Principales

### Cliente de Consola (`test_local_llms.py`)
- SelecciÃ³n interactiva de modelos
- ConfiguraciÃ³n de parÃ¡metros de generaciÃ³n
- ExportaciÃ³n de conversaciones
- EstadÃ­sticas de uso

### Interfaz Web
- Chat en tiempo real
- GestiÃ³n de sesiones
- ExportaciÃ³n de conversaciones
- Interfaz adaptativa (claro/oscuro)

## âš™ï¸ ConfiguraciÃ³n

Los parÃ¡metros de generaciÃ³n se pueden ajustar a travÃ©s de presets predefinidos o manualmente:

```python
{
    'temperature': 0.7,
    'max_tokens': 2000,
    'top_p': 1,
    'frequency_penalty': 0,
    'presence_penalty': 0
}
```

## ğŸ“Š MÃ©tricas y EstadÃ­sticas

El sistema registra:
- Tokens totales consumidos
- Tiempo de respuesta promedio
- NÃºmero total de mensajes
- DuraciÃ³n total de la sesiÃ³n

## ğŸ”„ Estado del Proyecto

Actualmente en desarrollo activo con enfoque en:
- OptimizaciÃ³n de la interfaz web
- Mejora del sistema de persistencia
- EvaluaciÃ³n de diferentes modelos
- DocumentaciÃ³n de mejores prÃ¡cticas

## ğŸ“œ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - vea el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

La Licencia MIT es una licencia de software permisiva que permite:
- âœ”ï¸ Uso comercial
- âœ”ï¸ ModificaciÃ³n
- âœ”ï¸ DistribuciÃ³n
- âœ”ï¸ Uso privado

Solo requiere mantener el aviso de copyright y la licencia en cualquier copia o parte sustancial del Software.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, abrir un issue para discutir cambios mayores.

## âœ¨ Agradecimientos

- Equipo de GPT4All por su excelente framework
- Comunidad de desarrolladores de modelos de lenguaje locales
