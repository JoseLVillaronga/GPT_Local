2025-01-29 13:40:04,269 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:40:05,020 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 13:40:32,840 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:40:36,546 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:40:55,937 - main - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 13:40:55,937 - main - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 13:41:52,292 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 13:41:56,413 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:42:14,848 - main - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 13:42:14,849 - main - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 13:42:28,920 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:42:29,188 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 13:44:24,261 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:44:24,488 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 13:44:59,798 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:45:04,084 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:48:58,891 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:49:02,877 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:51:27,668 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:51:29,273 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:57:30,123 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:57:30,511 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:01:33,308 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Ghost 7B v0.9.1
2025-01-29 14:01:33,308 - main - INFO - Creando nueva sesión con ID: 4871c054-0bdf-408a-9dd4-58ffe2f277ff
2025-01-29 14:01:33,416 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:01:41,903 - main - INFO - Nueva solicitud de chat - Sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff, Modelo: Ghost 7B v0.9.1
2025-01-29 14:01:43,392 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:02:12,419 - main - INFO - Nueva solicitud de chat - Sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff, Modelo: Ghost 7B v0.9.1
2025-01-29 14:02:16,273 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:02:49,913 - main - INFO - Nueva solicitud de chat - Sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff, Modelo: Ghost 7B v0.9.1
2025-01-29 14:02:51,188 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:03:13,280 - main - INFO - Intentando exportar conversación para sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff
2025-01-29 14:03:13,280 - main - INFO - Exportando conversación con modelo Ghost 7B v0.9.1 y servicio gpt4all
2025-01-29 14:03:13,281 - main - INFO - Conversación exportada exitosamente a: chat_history/gpt4all_Ghost 7B v0.9.1_20250129_140133.md
2025-01-29 14:04:09,696 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 14:04:10,187 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:23:37,457 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Wizard v1.2
2025-01-29 14:23:37,458 - __main__ - INFO - Creando nueva sesión con ID: ac36e8b6-b768-4f09-9bf3-54e4100b69d2
2025-01-29 14:23:46,262 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:24:04,637 - __main__ - INFO - Nueva solicitud de chat - Sesión: ac36e8b6-b768-4f09-9bf3-54e4100b69d2, Modelo: Wizard v1.2
2025-01-29 14:24:14,260 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:24:22,018 - __main__ - INFO - Intentando exportar conversación para sesión: ac36e8b6-b768-4f09-9bf3-54e4100b69d2
2025-01-29 14:24:22,019 - __main__ - INFO - Exportando conversación con modelo Wizard v1.2 y servicio gpt4all
2025-01-29 14:24:22,019 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Wizard v1.2_20250129_142337.md
2025-01-29 14:26:00,591 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 14:26:00,592 - __main__ - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 14:29:22,812 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 14:29:23,678 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:31:58,971 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Nomic Embed Text v1.5
2025-01-29 14:31:58,971 - __main__ - INFO - Creando nueva sesión con ID: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f
2025-01-29 14:32:00,544 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:32:21,131 - __main__ - INFO - Nueva solicitud de chat - Sesión: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:32:22,883 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:33:01,972 - __main__ - INFO - Nueva solicitud de chat - Sesión: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:33:03,057 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:33:36,768 - __main__ - INFO - Intentando exportar conversación para sesión: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f
2025-01-29 14:33:36,768 - __main__ - INFO - Exportando conversación con modelo Nomic Embed Text v1.5 y servicio gpt4all
2025-01-29 14:33:36,768 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Nomic Embed Text v1.5_20250129_143158.md
2025-01-29 14:33:36,778 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 14:33:36,779 - __main__ - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 14:33:36,786 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 14:33:36,786 - __main__ - INFO - Creando nueva sesión con ID: 60549453-6981-48bf-9812-5c7c95182118
2025-01-29 14:33:37,235 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:34:36,979 - __main__ - INFO - Nueva solicitud de chat - Sesión: 60549453-6981-48bf-9812-5c7c95182118, Modelo: Llama 3 8B Instruct
2025-01-29 14:34:42,033 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:35:34,646 - __main__ - INFO - Intentando exportar conversación para sesión: 60549453-6981-48bf-9812-5c7c95182118
2025-01-29 14:35:34,646 - __main__ - INFO - Exportando conversación con modelo Llama 3 8B Instruct y servicio GPT4All
2025-01-29 14:35:34,647 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_143336.md
2025-01-29 14:35:34,657 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Nomic Embed Text v1.5_20250129_143158.md
2025-01-29 14:35:34,658 - __main__ - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 14:35:34,665 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Nomic Embed Text v1.5
2025-01-29 14:35:34,665 - __main__ - INFO - Creando nueva sesión con ID: 740046b5-cc71-4720-84bd-a69f9ff7c21f
2025-01-29 14:35:35,251 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:36:22,966 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:36:23,965 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:36:52,553 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:36:53,676 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:38:56,051 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:38:58,397 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:41:58,207 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:42:00,453 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:43:06,058 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_143336.md
2025-01-29 14:43:06,058 - __main__ - INFO - Conversación cargada exitosamente: 3 mensajes
2025-01-29 14:43:06,064 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 14:43:06,065 - __main__ - INFO - Creando nueva sesión con ID: e2cdfd30-81f6-4d5c-95d7-c9150247f40a
2025-01-29 14:43:06,411 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:48:46,912 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 14:48:47,993 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:50:05,781 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: MPT Chat
2025-01-29 14:50:05,781 - __main__ - INFO - Creando nueva sesión con ID: 2df31ce9-24d2-48c9-bebc-6580650c7db9
2025-01-29 14:50:05,888 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:50:21,128 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2df31ce9-24d2-48c9-bebc-6580650c7db9, Modelo: MPT Chat
2025-01-29 14:50:23,193 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:17:22,964 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 15:17:24,076 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 15:17:54,252 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Reasoner v1
2025-01-29 15:17:54,252 - __main__ - INFO - Creando nueva sesión con ID: 7244d6a2-9914-4f80-a69c-5d8e65a80da5
2025-01-29 15:17:57,750 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:25:33,418 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 15:25:34,071 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 15:25:53,545 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: GPT4All Falcon
2025-01-29 15:25:53,546 - __main__ - INFO - Creando nueva sesión con ID: a47c5944-2b9b-419d-98a4-4f75473ff81c
2025-01-29 15:25:56,577 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:26:21,112 - __main__ - INFO - Nueva solicitud de chat - Sesión: a47c5944-2b9b-419d-98a4-4f75473ff81c, Modelo: GPT4All Falcon
2025-01-29 15:26:21,303 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:26:32,288 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 15:26:33,147 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 15:26:59,901 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-29 15:26:59,902 - __main__ - INFO - Creando nueva sesión con ID: 035e824a-9f6b-408d-8ce1-9b43057d405c
2025-01-29 15:27:01,499 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:27:09,567 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:27:11,757 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:28:02,862 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:28:08,007 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:30:08,614 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:30:12,960 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:45:00,877 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:45:06,571 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:47:48,739 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:47:56,842 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:51:03,815 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:51:06,211 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:51:57,624 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:52:00,770 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:54:08,318 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:54:12,801 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:01:21,589 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:01:27,839 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:04:11,530 - __main__ - INFO - Intentando exportar conversación para sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c
2025-01-29 16:04:11,530 - __main__ - INFO - Exportando conversación con modelo Phi-3 Mini Instruct y servicio gpt4all
2025-01-29 16:04:11,530 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Phi-3 Mini Instruct_20250129_152659.md
2025-01-29 16:06:54,550 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:07:01,004 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:09:04,605 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:09:05,681 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:09:10,898 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:09:10,959 - __main__ - ERROR - Error en chat: Error 500: 
2025-01-29 16:10:10,915 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:10:17,542 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:11:12,390 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:11:15,423 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:27:57,387 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:28:02,901 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:42:03,752 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:42:04,465 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:42:05,497 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:42:05,739 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:43:18,547 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-29 17:43:18,548 - __main__ - INFO - Creando nueva sesión con ID: 9c9e7202-b14b-4afd-aea9-8aeee212b631
2025-01-29 17:43:25,369 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:44:34,150 - __main__ - INFO - Nueva solicitud de chat - Sesión: 9c9e7202-b14b-4afd-aea9-8aeee212b631, Modelo: Phi-3 Mini Instruct
2025-01-29 17:44:41,446 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:53:48,135 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:53:48,518 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:57:28,573 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:57:28,983 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:57:34,221 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:57:34,511 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:57:49,260 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Reasoner v1
2025-01-29 17:57:49,260 - __main__ - INFO - Creando nueva sesión con ID: fa7207cf-35af-4091-9ccc-96a07a9814bb
2025-01-29 17:57:50,842 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:59:31,705 - __main__ - INFO - Nueva solicitud de chat - Sesión: fa7207cf-35af-4091-9ccc-96a07a9814bb, Modelo: Reasoner v1
2025-01-29 17:59:34,277 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:00:55,106 - __main__ - INFO - Nueva solicitud de chat - Sesión: fa7207cf-35af-4091-9ccc-96a07a9814bb, Modelo: Reasoner v1
2025-01-29 18:00:59,904 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:02:48,936 - __main__ - INFO - Nueva solicitud de chat - Sesión: fa7207cf-35af-4091-9ccc-96a07a9814bb, Modelo: Reasoner v1
2025-01-29 18:02:51,749 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:05:00,736 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:05:01,077 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:05:13,195 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-29 18:05:13,195 - __main__ - INFO - Creando nueva sesión con ID: d37551ae-0748-4533-be3b-f91952abbedd
2025-01-29 18:05:18,522 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:05:34,498 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:05:34,792 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:05:55,891 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:05:57,043 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:06:52,053 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:06:52,855 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:07:51,285 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:07:51,908 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:08:22,855 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:08:23,625 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:09:13,745 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:09:15,421 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:09:51,552 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:09:52,224 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:10:20,799 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:10:24,226 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:12:11,760 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:12:14,281 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:13:08,695 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:13:11,178 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:15:22,429 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:15:28,462 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:17:38,296 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:17:43,333 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:19:02,771 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:19:04,665 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:54:51,284 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:54:51,771 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:02,573 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:02,788 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:04,485 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:04,792 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:37,016 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:37,268 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:38,910 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:39,176 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:59:19,807 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:59:20,049 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:59:21,485 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:59:21,681 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:59:46,150 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:59:46,394 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:00:35,386 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:00:35,386 - __main__ - INFO - Creando nueva sesión con ID: 8920b845-be14-42d2-86a6-bf2557ba07c0
2025-01-29 19:00:38,955 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:01:09,130 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:01:09,828 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:02:02,946 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:02:03,435 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:02:21,573 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:02:22,157 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:03:20,794 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:03:24,723 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:05:00,741 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:05:04,859 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:06:09,662 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:06:14,210 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:09:33,024 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:09:38,576 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:11:29,002 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:11:33,652 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:13:58,718 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:14:03,114 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:16:33,099 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:16:34,604 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:16:58,552 - __main__ - INFO - Intentando exportar conversación para sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0
2025-01-29 19:16:58,553 - __main__ - INFO - Exportando conversación con modelo Qwen2-1.5B-Instruct y servicio gpt4all
2025-01-29 19:16:58,560 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 19:20:05,920 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:20:06,248 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:20:44,701 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 19:20:44,702 - __main__ - INFO - Conversación cargada exitosamente: 22 mensajes
2025-01-29 19:20:44,711 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:20:44,711 - __main__ - INFO - Creando nueva sesión con ID: 66d20c6d-6ad7-4a02-8486-c19aa4bed2b1
2025-01-29 19:20:45,368 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:21:26,852 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:21:27,113 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:21:40,854 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-29 19:21:40,854 - __main__ - INFO - Creando nueva sesión con ID: 882988c4-b247-48b8-a63b-a4d4a7e3cf61
2025-01-29 19:21:43,440 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:22:46,484 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:22:48,721 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:23:38,711 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:23:42,523 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:24:31,286 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:24:35,033 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:25:51,603 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:25:52,962 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:26:46,539 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:26:47,064 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:26:49,903 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:26:50,097 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 20:23:50,979 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 20:23:51,405 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 20:25:30,507 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 20:25:30,782 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 20:29:48,446 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 20:29:49,148 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:23:40,187 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:23:40,694 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:25:50,011 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:25:50,203 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:26:23,192 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:26:23,387 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:28:30,849 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:28:31,142 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:29:50,017 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:29:50,548 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:31:55,504 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:31:55,894 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:38:03,637 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:38:04,027 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:40:21,366 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:40:21,621 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:40:48,959 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:40:49,235 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:42:33,141 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:42:33,442 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:47:33,234 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 21:47:33,235 - __main__ - INFO - Conversación cargada exitosamente: 22 mensajes
2025-01-29 21:47:33,243 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 21:47:33,244 - __main__ - INFO - Creando nueva sesión con ID: f1d085aa-b385-4340-a58c-92505d7e6499
2025-01-29 21:47:35,100 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 21:48:39,225 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Phi-3 Mini Instruct_20250129_152659.md
2025-01-29 21:48:39,226 - __main__ - INFO - Conversación cargada exitosamente: 20 mensajes
2025-01-29 21:48:39,233 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-29 21:48:39,233 - __main__ - INFO - Creando nueva sesión con ID: 4a9582a1-59fe-431b-9f06-67993e0a60f7
2025-01-29 21:48:39,420 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 21:55:08,184 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:55:08,889 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:55:25,790 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:55:25,995 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:58:33,111 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:58:33,365 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:58:40,498 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 21:58:40,499 - __main__ - INFO - Conversación cargada exitosamente: 22 mensajes
2025-01-29 21:58:40,509 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 21:58:40,513 - __main__ - INFO - Creando nueva sesión con ID: ffd457a8-d100-4c33-91a8-9d5a0632ede6
2025-01-29 21:58:40,748 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:08:31,102 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 22:08:31,576 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 22:08:45,749 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 22:08:45,749 - __main__ - INFO - Creando nueva sesión con ID: 5311f7c1-b2b2-47dc-aecc-ae2a14aec3ce
2025-01-29 22:08:45,884 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:09:04,460 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5311f7c1-b2b2-47dc-aecc-ae2a14aec3ce, Modelo: Llama 3 8B Instruct
2025-01-29 22:09:08,027 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:09:55,204 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5311f7c1-b2b2-47dc-aecc-ae2a14aec3ce, Modelo: Llama 3 8B Instruct
2025-01-29 22:10:01,126 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:56:46,845 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 22:56:47,482 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 22:57:31,892 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:57:31,892 - __main__ - INFO - Creando nueva sesión con ID: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931
2025-01-29 22:57:35,709 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:57:51,266 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:57:52,310 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:58:53,774 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:58:55,138 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:59:49,696 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:59:51,005 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:00:33,446 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:00:35,439 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:01:29,150 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:01:33,829 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:03:29,769 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:03:36,085 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:05:21,031 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:05:28,657 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:07:31,740 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:07:34,154 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:11:06,307 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:11:13,616 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:17:50,685 - __main__ - INFO - Intentando exportar conversación para sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931
2025-01-29 23:17:50,686 - __main__ - INFO - Exportando conversación con modelo Nous Hermes 2 Mistral DPO y servicio gpt4all
2025-01-29 23:17:50,692 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Nous Hermes 2 Mistral DPO_20250129_225731.md
2025-01-30 00:50:48,759 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 00:50:48,941 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 00:51:24,137 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: GPT4All Falcon
2025-01-30 00:51:24,138 - __main__ - INFO - Creando nueva sesión con ID: f4bd9798-7b6d-46f3-89ef-39a40d8b62e3
2025-01-30 00:51:27,461 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:51:41,634 - __main__ - INFO - Nueva solicitud de chat - Sesión: f4bd9798-7b6d-46f3-89ef-39a40d8b62e3, Modelo: GPT4All Falcon
2025-01-30 00:51:41,870 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:51:55,203 - __main__ - INFO - Nueva solicitud de chat - Sesión: f4bd9798-7b6d-46f3-89ef-39a40d8b62e3, Modelo: GPT4All Falcon
2025-01-30 00:51:55,369 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:52:10,077 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 00:52:10,291 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 00:52:23,003 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-30 00:52:23,003 - __main__ - INFO - Creando nueva sesión con ID: 1357bb6f-ea19-42f6-a015-181242d5566c
2025-01-30 00:52:24,577 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:52:31,541 - __main__ - INFO - Nueva solicitud de chat - Sesión: 1357bb6f-ea19-42f6-a015-181242d5566c, Modelo: Llama 3 8B Instruct
2025-01-30 00:52:33,587 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:56:21,714 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 00:56:22,202 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 00:56:43,366 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-30 00:56:43,366 - __main__ - INFO - Creando nueva sesión con ID: 780f56f6-bd3d-438b-9de7-e44340ccb4be
2025-01-30 00:56:48,897 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:56:57,935 - __main__ - INFO - Nueva solicitud de chat - Sesión: 780f56f6-bd3d-438b-9de7-e44340ccb4be, Modelo: Orca 2 (Full)
2025-01-30 00:56:59,005 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 01:01:43,804 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 01:01:44,246 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 01:02:22,363 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 01:02:22,363 - __main__ - INFO - Creando nueva sesión con ID: b40ae6f4-53d6-4f53-b0f4-96110e1fa6e6
2025-01-30 01:02:25,883 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 01:02:35,788 - __main__ - INFO - Nueva solicitud de chat - Sesión: b40ae6f4-53d6-4f53-b0f4-96110e1fa6e6, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 01:02:37,539 - __main__ - INFO - Respuesta generada exitosamente
