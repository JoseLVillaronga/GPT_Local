2025-01-29 13:40:04,269 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:40:05,020 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 13:40:32,840 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:40:36,546 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:40:55,937 - main - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 13:40:55,937 - main - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 13:41:52,292 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 13:41:56,413 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:42:14,848 - main - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 13:42:14,849 - main - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 13:42:28,920 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:42:29,188 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 13:44:24,261 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:44:24,488 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 13:44:59,798 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:45:04,084 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:48:58,891 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:49:02,877 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:51:27,668 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-29 13:51:29,273 - main - INFO - Respuesta generada exitosamente
2025-01-29 13:57:30,123 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 13:57:30,511 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:01:33,308 - main - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Ghost 7B v0.9.1
2025-01-29 14:01:33,308 - main - INFO - Creando nueva sesión con ID: 4871c054-0bdf-408a-9dd4-58ffe2f277ff
2025-01-29 14:01:33,416 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:01:41,903 - main - INFO - Nueva solicitud de chat - Sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff, Modelo: Ghost 7B v0.9.1
2025-01-29 14:01:43,392 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:02:12,419 - main - INFO - Nueva solicitud de chat - Sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff, Modelo: Ghost 7B v0.9.1
2025-01-29 14:02:16,273 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:02:49,913 - main - INFO - Nueva solicitud de chat - Sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff, Modelo: Ghost 7B v0.9.1
2025-01-29 14:02:51,188 - main - INFO - Respuesta generada exitosamente
2025-01-29 14:03:13,280 - main - INFO - Intentando exportar conversación para sesión: 4871c054-0bdf-408a-9dd4-58ffe2f277ff
2025-01-29 14:03:13,280 - main - INFO - Exportando conversación con modelo Ghost 7B v0.9.1 y servicio gpt4all
2025-01-29 14:03:13,281 - main - INFO - Conversación exportada exitosamente a: chat_history/gpt4all_Ghost 7B v0.9.1_20250129_140133.md
2025-01-29 14:04:09,696 - main - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 14:04:10,187 - main - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:23:37,457 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Wizard v1.2
2025-01-29 14:23:37,458 - __main__ - INFO - Creando nueva sesión con ID: ac36e8b6-b768-4f09-9bf3-54e4100b69d2
2025-01-29 14:23:46,262 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:24:04,637 - __main__ - INFO - Nueva solicitud de chat - Sesión: ac36e8b6-b768-4f09-9bf3-54e4100b69d2, Modelo: Wizard v1.2
2025-01-29 14:24:14,260 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:24:22,018 - __main__ - INFO - Intentando exportar conversación para sesión: ac36e8b6-b768-4f09-9bf3-54e4100b69d2
2025-01-29 14:24:22,019 - __main__ - INFO - Exportando conversación con modelo Wizard v1.2 y servicio gpt4all
2025-01-29 14:24:22,019 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Wizard v1.2_20250129_142337.md
2025-01-29 14:26:00,591 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 14:26:00,592 - __main__ - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 14:29:22,812 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 14:29:23,678 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:31:58,971 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Nomic Embed Text v1.5
2025-01-29 14:31:58,971 - __main__ - INFO - Creando nueva sesión con ID: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f
2025-01-29 14:32:00,544 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:32:21,131 - __main__ - INFO - Nueva solicitud de chat - Sesión: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:32:22,883 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:33:01,972 - __main__ - INFO - Nueva solicitud de chat - Sesión: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:33:03,057 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:33:36,768 - __main__ - INFO - Intentando exportar conversación para sesión: 497fc89a-b6c7-4822-a03b-bf4fba43ab7f
2025-01-29 14:33:36,768 - __main__ - INFO - Exportando conversación con modelo Nomic Embed Text v1.5 y servicio gpt4all
2025-01-29 14:33:36,768 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Nomic Embed Text v1.5_20250129_143158.md
2025-01-29 14:33:36,778 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_010236.md
2025-01-29 14:33:36,779 - __main__ - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 14:33:36,786 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 14:33:36,786 - __main__ - INFO - Creando nueva sesión con ID: 60549453-6981-48bf-9812-5c7c95182118
2025-01-29 14:33:37,235 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:34:36,979 - __main__ - INFO - Nueva solicitud de chat - Sesión: 60549453-6981-48bf-9812-5c7c95182118, Modelo: Llama 3 8B Instruct
2025-01-29 14:34:42,033 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:35:34,646 - __main__ - INFO - Intentando exportar conversación para sesión: 60549453-6981-48bf-9812-5c7c95182118
2025-01-29 14:35:34,646 - __main__ - INFO - Exportando conversación con modelo Llama 3 8B Instruct y servicio GPT4All
2025-01-29 14:35:34,647 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_143336.md
2025-01-29 14:35:34,657 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Nomic Embed Text v1.5_20250129_143158.md
2025-01-29 14:35:34,658 - __main__ - INFO - Conversación cargada exitosamente: 6 mensajes
2025-01-29 14:35:34,665 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Nomic Embed Text v1.5
2025-01-29 14:35:34,665 - __main__ - INFO - Creando nueva sesión con ID: 740046b5-cc71-4720-84bd-a69f9ff7c21f
2025-01-29 14:35:35,251 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:36:22,966 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:36:23,965 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:36:52,553 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:36:53,676 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:38:56,051 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:38:58,397 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:41:58,207 - __main__ - INFO - Nueva solicitud de chat - Sesión: 740046b5-cc71-4720-84bd-a69f9ff7c21f, Modelo: Nomic Embed Text v1.5
2025-01-29 14:42:00,453 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:43:06,058 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/GPT4All_Llama 3 8B Instruct_20250129_143336.md
2025-01-29 14:43:06,058 - __main__ - INFO - Conversación cargada exitosamente: 3 mensajes
2025-01-29 14:43:06,064 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 14:43:06,065 - __main__ - INFO - Creando nueva sesión con ID: e2cdfd30-81f6-4d5c-95d7-c9150247f40a
2025-01-29 14:43:06,411 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:48:46,912 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 14:48:47,993 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 14:50:05,781 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: MPT Chat
2025-01-29 14:50:05,781 - __main__ - INFO - Creando nueva sesión con ID: 2df31ce9-24d2-48c9-bebc-6580650c7db9
2025-01-29 14:50:05,888 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 14:50:21,128 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2df31ce9-24d2-48c9-bebc-6580650c7db9, Modelo: MPT Chat
2025-01-29 14:50:23,193 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:17:22,964 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 15:17:24,076 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 15:17:54,252 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Reasoner v1
2025-01-29 15:17:54,252 - __main__ - INFO - Creando nueva sesión con ID: 7244d6a2-9914-4f80-a69c-5d8e65a80da5
2025-01-29 15:17:57,750 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:25:33,418 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 15:25:34,071 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 15:25:53,545 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: GPT4All Falcon
2025-01-29 15:25:53,546 - __main__ - INFO - Creando nueva sesión con ID: a47c5944-2b9b-419d-98a4-4f75473ff81c
2025-01-29 15:25:56,577 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:26:21,112 - __main__ - INFO - Nueva solicitud de chat - Sesión: a47c5944-2b9b-419d-98a4-4f75473ff81c, Modelo: GPT4All Falcon
2025-01-29 15:26:21,303 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:26:32,288 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 15:26:33,147 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 15:26:59,901 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-29 15:26:59,902 - __main__ - INFO - Creando nueva sesión con ID: 035e824a-9f6b-408d-8ce1-9b43057d405c
2025-01-29 15:27:01,499 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:27:09,567 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:27:11,757 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:28:02,862 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:28:08,007 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:30:08,614 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:30:12,960 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:45:00,877 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:45:06,571 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:47:48,739 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:47:56,842 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:51:03,815 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:51:06,211 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:51:57,624 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:52:00,770 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 15:54:08,318 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 15:54:12,801 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:01:21,589 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:01:27,839 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:04:11,530 - __main__ - INFO - Intentando exportar conversación para sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c
2025-01-29 16:04:11,530 - __main__ - INFO - Exportando conversación con modelo Phi-3 Mini Instruct y servicio gpt4all
2025-01-29 16:04:11,530 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Phi-3 Mini Instruct_20250129_152659.md
2025-01-29 16:06:54,550 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:07:01,004 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:09:04,605 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:09:05,681 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:09:10,898 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:09:10,959 - __main__ - ERROR - Error en chat: Error 500: 
2025-01-29 16:10:10,915 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:10:17,542 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:11:12,390 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:11:15,423 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 16:27:57,387 - __main__ - INFO - Nueva solicitud de chat - Sesión: 035e824a-9f6b-408d-8ce1-9b43057d405c, Modelo: Phi-3 Mini Instruct
2025-01-29 16:28:02,901 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:42:03,752 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:42:04,465 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:42:05,497 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:42:05,739 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:43:18,547 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-29 17:43:18,548 - __main__ - INFO - Creando nueva sesión con ID: 9c9e7202-b14b-4afd-aea9-8aeee212b631
2025-01-29 17:43:25,369 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:44:34,150 - __main__ - INFO - Nueva solicitud de chat - Sesión: 9c9e7202-b14b-4afd-aea9-8aeee212b631, Modelo: Phi-3 Mini Instruct
2025-01-29 17:44:41,446 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:53:48,135 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:53:48,518 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:57:28,573 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:57:28,983 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:57:34,221 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 17:57:34,511 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 17:57:49,260 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Reasoner v1
2025-01-29 17:57:49,260 - __main__ - INFO - Creando nueva sesión con ID: fa7207cf-35af-4091-9ccc-96a07a9814bb
2025-01-29 17:57:50,842 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 17:59:31,705 - __main__ - INFO - Nueva solicitud de chat - Sesión: fa7207cf-35af-4091-9ccc-96a07a9814bb, Modelo: Reasoner v1
2025-01-29 17:59:34,277 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:00:55,106 - __main__ - INFO - Nueva solicitud de chat - Sesión: fa7207cf-35af-4091-9ccc-96a07a9814bb, Modelo: Reasoner v1
2025-01-29 18:00:59,904 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:02:48,936 - __main__ - INFO - Nueva solicitud de chat - Sesión: fa7207cf-35af-4091-9ccc-96a07a9814bb, Modelo: Reasoner v1
2025-01-29 18:02:51,749 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:05:00,736 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:05:01,077 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:05:13,195 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-29 18:05:13,195 - __main__ - INFO - Creando nueva sesión con ID: d37551ae-0748-4533-be3b-f91952abbedd
2025-01-29 18:05:18,522 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:05:34,498 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:05:34,792 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:05:55,891 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:05:57,043 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:06:52,053 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:06:52,855 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:07:51,285 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:07:51,908 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:08:22,855 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:08:23,625 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:09:13,745 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:09:15,421 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:09:51,552 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:09:52,224 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:10:20,799 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:10:24,226 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:12:11,760 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:12:14,281 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:13:08,695 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:13:11,178 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:15:22,429 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:15:28,462 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:17:38,296 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:17:43,333 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:19:02,771 - __main__ - INFO - Nueva solicitud de chat - Sesión: d37551ae-0748-4533-be3b-f91952abbedd, Modelo: Orca 2 (Full)
2025-01-29 18:19:04,665 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 18:54:51,284 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:54:51,771 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:02,573 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:02,788 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:04,485 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:04,792 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:37,016 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:37,268 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:57:38,910 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:57:39,176 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:59:19,807 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:59:20,049 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:59:21,485 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:59:21,681 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 18:59:46,150 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 18:59:46,394 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:00:35,386 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:00:35,386 - __main__ - INFO - Creando nueva sesión con ID: 8920b845-be14-42d2-86a6-bf2557ba07c0
2025-01-29 19:00:38,955 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:01:09,130 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:01:09,828 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:02:02,946 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:02:03,435 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:02:21,573 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:02:22,157 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:03:20,794 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:03:24,723 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:05:00,741 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:05:04,859 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:06:09,662 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:06:14,210 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:09:33,024 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:09:38,576 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:11:29,002 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:11:33,652 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:13:58,718 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:14:03,114 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:16:33,099 - __main__ - INFO - Nueva solicitud de chat - Sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:16:34,604 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:16:58,552 - __main__ - INFO - Intentando exportar conversación para sesión: 8920b845-be14-42d2-86a6-bf2557ba07c0
2025-01-29 19:16:58,553 - __main__ - INFO - Exportando conversación con modelo Qwen2-1.5B-Instruct y servicio gpt4all
2025-01-29 19:16:58,560 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 19:20:05,920 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:20:06,248 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:20:44,701 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 19:20:44,702 - __main__ - INFO - Conversación cargada exitosamente: 22 mensajes
2025-01-29 19:20:44,711 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 19:20:44,711 - __main__ - INFO - Creando nueva sesión con ID: 66d20c6d-6ad7-4a02-8486-c19aa4bed2b1
2025-01-29 19:20:45,368 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:21:26,852 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:21:27,113 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:21:40,854 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-29 19:21:40,854 - __main__ - INFO - Creando nueva sesión con ID: 882988c4-b247-48b8-a63b-a4d4a7e3cf61
2025-01-29 19:21:43,440 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:22:46,484 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:22:48,721 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:23:38,711 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:23:42,523 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:24:31,286 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:24:35,033 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:25:51,603 - __main__ - INFO - Nueva solicitud de chat - Sesión: 882988c4-b247-48b8-a63b-a4d4a7e3cf61, Modelo: Orca 2 (Full)
2025-01-29 19:25:52,962 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 19:26:46,539 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:26:47,064 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 19:26:49,903 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 19:26:50,097 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 20:23:50,979 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 20:23:51,405 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 20:25:30,507 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 20:25:30,782 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 20:29:48,446 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 20:29:49,148 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:23:40,187 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:23:40,694 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:25:50,011 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:25:50,203 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:26:23,192 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:26:23,387 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:28:30,849 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:28:31,142 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:29:50,017 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:29:50,548 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:31:55,504 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:31:55,894 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:38:03,637 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:38:04,027 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:40:21,366 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:40:21,621 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:40:48,959 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:40:49,235 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:42:33,141 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:42:33,442 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:47:33,234 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 21:47:33,235 - __main__ - INFO - Conversación cargada exitosamente: 22 mensajes
2025-01-29 21:47:33,243 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 21:47:33,244 - __main__ - INFO - Creando nueva sesión con ID: f1d085aa-b385-4340-a58c-92505d7e6499
2025-01-29 21:47:35,100 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 21:48:39,225 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Phi-3 Mini Instruct_20250129_152659.md
2025-01-29 21:48:39,226 - __main__ - INFO - Conversación cargada exitosamente: 20 mensajes
2025-01-29 21:48:39,233 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-29 21:48:39,233 - __main__ - INFO - Creando nueva sesión con ID: 4a9582a1-59fe-431b-9f06-67993e0a60f7
2025-01-29 21:48:39,420 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 21:55:08,184 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:55:08,889 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:55:25,790 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:55:25,995 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:58:33,111 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 21:58:33,365 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 21:58:40,498 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Qwen2-1.5B-Instruct_20250129_190035.md
2025-01-29 21:58:40,499 - __main__ - INFO - Conversación cargada exitosamente: 22 mensajes
2025-01-29 21:58:40,509 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-01-29 21:58:40,513 - __main__ - INFO - Creando nueva sesión con ID: ffd457a8-d100-4c33-91a8-9d5a0632ede6
2025-01-29 21:58:40,748 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:08:31,102 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 22:08:31,576 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 22:08:45,749 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-29 22:08:45,749 - __main__ - INFO - Creando nueva sesión con ID: 5311f7c1-b2b2-47dc-aecc-ae2a14aec3ce
2025-01-29 22:08:45,884 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:09:04,460 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5311f7c1-b2b2-47dc-aecc-ae2a14aec3ce, Modelo: Llama 3 8B Instruct
2025-01-29 22:09:08,027 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:09:55,204 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5311f7c1-b2b2-47dc-aecc-ae2a14aec3ce, Modelo: Llama 3 8B Instruct
2025-01-29 22:10:01,126 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:56:46,845 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-29 22:56:47,482 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-29 22:57:31,892 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:57:31,892 - __main__ - INFO - Creando nueva sesión con ID: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931
2025-01-29 22:57:35,709 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:57:51,266 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:57:52,310 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:58:53,774 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:58:55,138 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 22:59:49,696 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 22:59:51,005 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:00:33,446 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:00:35,439 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:01:29,150 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:01:33,829 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:03:29,769 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:03:36,085 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:05:21,031 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:05:28,657 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:07:31,740 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:07:34,154 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:11:06,307 - __main__ - INFO - Nueva solicitud de chat - Sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931, Modelo: Nous Hermes 2 Mistral DPO
2025-01-29 23:11:13,616 - __main__ - INFO - Respuesta generada exitosamente
2025-01-29 23:17:50,685 - __main__ - INFO - Intentando exportar conversación para sesión: e24257b1-c2e9-4df8-bbd3-5e9ef5e63931
2025-01-29 23:17:50,686 - __main__ - INFO - Exportando conversación con modelo Nous Hermes 2 Mistral DPO y servicio gpt4all
2025-01-29 23:17:50,692 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Nous Hermes 2 Mistral DPO_20250129_225731.md
2025-01-30 00:50:48,759 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 00:50:48,941 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 00:51:24,137 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: GPT4All Falcon
2025-01-30 00:51:24,138 - __main__ - INFO - Creando nueva sesión con ID: f4bd9798-7b6d-46f3-89ef-39a40d8b62e3
2025-01-30 00:51:27,461 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:51:41,634 - __main__ - INFO - Nueva solicitud de chat - Sesión: f4bd9798-7b6d-46f3-89ef-39a40d8b62e3, Modelo: GPT4All Falcon
2025-01-30 00:51:41,870 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:51:55,203 - __main__ - INFO - Nueva solicitud de chat - Sesión: f4bd9798-7b6d-46f3-89ef-39a40d8b62e3, Modelo: GPT4All Falcon
2025-01-30 00:51:55,369 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:52:10,077 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 00:52:10,291 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 00:52:23,003 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3 8B Instruct
2025-01-30 00:52:23,003 - __main__ - INFO - Creando nueva sesión con ID: 1357bb6f-ea19-42f6-a015-181242d5566c
2025-01-30 00:52:24,577 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:52:31,541 - __main__ - INFO - Nueva solicitud de chat - Sesión: 1357bb6f-ea19-42f6-a015-181242d5566c, Modelo: Llama 3 8B Instruct
2025-01-30 00:52:33,587 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:56:21,714 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 00:56:22,202 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 00:56:43,366 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-30 00:56:43,366 - __main__ - INFO - Creando nueva sesión con ID: 780f56f6-bd3d-438b-9de7-e44340ccb4be
2025-01-30 00:56:48,897 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 00:56:57,935 - __main__ - INFO - Nueva solicitud de chat - Sesión: 780f56f6-bd3d-438b-9de7-e44340ccb4be, Modelo: Orca 2 (Full)
2025-01-30 00:56:59,005 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 01:01:43,804 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 01:01:44,246 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 01:02:22,363 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 01:02:22,363 - __main__ - INFO - Creando nueva sesión con ID: b40ae6f4-53d6-4f53-b0f4-96110e1fa6e6
2025-01-30 01:02:25,883 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 01:02:35,788 - __main__ - INFO - Nueva solicitud de chat - Sesión: b40ae6f4-53d6-4f53-b0f4-96110e1fa6e6, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 01:02:37,539 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 01:35:31,542 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 01:35:31,797 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 01:35:54,137 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 01:35:54,137 - __main__ - INFO - Creando nueva sesión con ID: 4d7af8de-52a6-4b3b-9466-f0a9ab2fbdca
2025-01-30 01:36:01,058 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 08:52:28,003 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 08:52:28,825 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 08:53:09,712 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 08:53:09,712 - __main__ - INFO - Creando nueva sesión con ID: 9912994e-2c60-4479-b345-36a08ca0643a
2025-01-30 08:53:15,745 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:11:04,824 - __main__ - INFO - Intentando exportar conversación para sesión: 9912994e-2c60-4479-b345-36a08ca0643a
2025-01-30 09:11:04,824 - __main__ - INFO - Exportando conversación con modelo Llama 3.1 8B Instruct 128k y servicio gpt4all
2025-01-30 09:11:04,825 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Llama 3.1 8B Instruct 128k_20250130_085309.md
2025-01-30 09:13:18,221 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 09:13:18,542 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 09:13:34,294 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-30 09:13:34,294 - __main__ - INFO - Creando nueva sesión con ID: 87a938a5-f361-4a94-b01c-dafb6f26e097
2025-01-30 09:13:38,501 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:19:21,739 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:19:26,149 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:20:47,606 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:20:53,171 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:24:55,577 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:25:09,797 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:28:17,873 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:28:23,243 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:29:20,824 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:29:25,816 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:33:11,079 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:33:13,276 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:36:42,395 - __main__ - INFO - Nueva solicitud de chat - Sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097, Modelo: Phi-3 Mini Instruct
2025-01-30 09:36:43,816 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:40:13,666 - __main__ - INFO - Intentando exportar conversación para sesión: 87a938a5-f361-4a94-b01c-dafb6f26e097
2025-01-30 09:40:13,666 - __main__ - INFO - Exportando conversación con modelo Phi-3 Mini Instruct y servicio gpt4all
2025-01-30 09:40:13,667 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Phi-3 Mini Instruct_20250130_091334.md
2025-01-30 09:46:57,133 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Phi-3 Mini Instruct_20250130_091334.md
2025-01-30 09:46:57,134 - __main__ - INFO - Conversación cargada exitosamente: 16 mensajes
2025-01-30 09:46:57,145 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-30 09:46:57,145 - __main__ - INFO - Creando nueva sesión con ID: aecfb3f1-aef2-484a-821f-6542b81b2fdd
2025-01-30 09:46:58,565 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:47:24,377 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 09:47:24,854 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 09:48:04,717 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Ghost 7B v0.9.1
2025-01-30 09:48:04,717 - __main__ - INFO - Creando nueva sesión con ID: f30f257f-65f9-4010-bf0b-943b569b38ca
2025-01-30 09:48:06,479 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:48:12,278 - __main__ - INFO - Nueva solicitud de chat - Sesión: f30f257f-65f9-4010-bf0b-943b569b38ca, Modelo: Ghost 7B v0.9.1
2025-01-30 09:48:18,049 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:50:15,833 - __main__ - INFO - Intentando exportar conversación para sesión: f30f257f-65f9-4010-bf0b-943b569b38ca
2025-01-30 09:50:15,833 - __main__ - INFO - Exportando conversación con modelo Ghost 7B v0.9.1 y servicio gpt4all
2025-01-30 09:50:15,833 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Ghost 7B v0.9.1_20250130_094804.md
2025-01-30 09:50:34,391 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 09:50:34,624 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 09:51:27,889 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-01-30 09:51:27,890 - __main__ - INFO - Creando nueva sesión con ID: cc182f9c-adde-4965-94c6-c1111ca37b98
2025-01-30 09:51:34,780 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 09:53:54,205 - __main__ - INFO - Nueva solicitud de chat - Sesión: cc182f9c-adde-4965-94c6-c1111ca37b98, Modelo: Starcoder
2025-01-30 09:54:00,578 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 10:01:05,029 - __main__ - INFO - Intentando exportar conversación para sesión: cc182f9c-adde-4965-94c6-c1111ca37b98
2025-01-30 10:01:05,029 - __main__ - INFO - Exportando conversación con modelo Starcoder y servicio gpt4all
2025-01-30 10:01:05,029 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Starcoder_20250130_095127.md
2025-01-30 10:01:18,267 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 10:01:18,965 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 10:02:08,973 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Starcoder_20250130_095127.md
2025-01-30 10:02:08,973 - __main__ - INFO - Conversación cargada exitosamente: 4 mensajes
2025-01-30 10:02:08,981 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-01-30 10:02:08,982 - __main__ - INFO - Creando nueva sesión con ID: 25d9b9b7-c3b3-494b-9687-8a8c1d483a20
2025-01-30 10:02:09,392 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:13:50,766 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 11:13:51,154 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 11:18:47,797 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 11:18:47,954 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 11:20:40,985 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 11:20:41,484 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 11:20:55,595 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-01-30 11:20:55,595 - __main__ - INFO - Creando nueva sesión con ID: 5c473436-e7cc-4b88-b5fc-2489bbfea03c
2025-01-30 11:20:57,695 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:21:45,026 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:21:45,872 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:22:33,130 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:22:36,163 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:23:46,687 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:23:53,607 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:25:13,924 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:25:21,574 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:26:27,052 - __main__ - INFO - Intentando exportar conversación para sesión: 25d9b9b7-c3b3-494b-9687-8a8c1d483a20
2025-01-30 11:26:27,052 - __main__ - INFO - Exportando conversación con modelo Starcoder y servicio gpt4all
2025-01-30 11:26:27,052 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Starcoder_20250130_100208.md
2025-01-30 11:26:27,073 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Starcoder_20250130_095127.md
2025-01-30 11:26:27,073 - __main__ - INFO - Conversación cargada exitosamente: 4 mensajes
2025-01-30 11:26:27,081 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-01-30 11:26:27,081 - __main__ - INFO - Creando nueva sesión con ID: 517b11aa-a63c-4252-8b7a-b9b356b25f64
2025-01-30 11:26:28,910 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:26:55,003 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:27:06,255 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:28:04,830 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:28:17,496 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:30:21,743 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:30:35,862 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:34:42,929 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:34:48,532 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:36:48,495 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:36:59,854 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:39:04,530 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:39:09,651 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:41:22,112 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:41:27,073 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:42:38,307 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c, Modelo: Orca 2 (Full)
2025-01-30 11:42:40,449 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 11:44:01,331 - __main__ - INFO - Intentando exportar conversación para sesión: 5c473436-e7cc-4b88-b5fc-2489bbfea03c
2025-01-30 11:44:01,331 - __main__ - INFO - Exportando conversación con modelo Orca 2 (Full) y servicio gpt4all
2025-01-30 11:44:01,332 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Orca 2 (Full)_20250130_112055.md
2025-01-30 11:45:31,738 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 11:45:32,217 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '4', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 12:07:34,679 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Orca 2 (Full)_20250130_112055.md
2025-01-30 12:07:34,679 - __main__ - INFO - Conversación cargada exitosamente: 26 mensajes
2025-01-30 12:07:34,691 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2
2025-01-30 12:07:34,691 - __main__ - INFO - Creando nueva sesión con ID: 0d2fda0e-d8fc-4be3-98f1-d2854af2a02a
2025-01-30 12:07:36,505 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:08:06,669 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Orca 2 (Full)_20250130_112055.md
2025-01-30 12:08:06,669 - __main__ - INFO - Conversación cargada exitosamente: 26 mensajes
2025-01-30 12:08:06,684 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2
2025-01-30 12:08:06,684 - __main__ - INFO - Creando nueva sesión con ID: 7522094b-8b38-4c1b-971d-43a289b5e936
2025-01-30 12:08:07,128 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:20:49,422 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:20:53,828 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:21:05,120 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:21:09,592 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:21:31,007 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:21:35,698 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:21:48,417 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:21:53,111 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:21:58,973 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:22:04,492 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:22:19,446 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:22:20,574 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:23:40,655 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:23:41,765 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:24:18,411 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:24:19,149 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:26:18,481 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:26:19,749 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:26:27,558 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:26:33,952 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:28:09,058 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:28:10,578 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:29:43,080 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:29:45,837 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:31:57,358 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:32:00,506 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:33:08,596 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:33:12,137 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:34:38,914 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7522094b-8b38-4c1b-971d-43a289b5e936, Modelo: Orca 2 (Full)
2025-01-30 12:34:40,382 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:35:27,570 - __main__ - INFO - Intentando exportar conversación para sesión: 7522094b-8b38-4c1b-971d-43a289b5e936
2025-01-30 12:35:27,570 - __main__ - INFO - Exportando conversación con modelo Orca 2 y servicio Full
2025-01-30 12:35:27,570 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/Full_Orca 2_20250130_120806.md
2025-01-30 12:36:23,205 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Starcoder_20250130_100208.md
2025-01-30 12:36:23,205 - __main__ - INFO - Conversación cargada exitosamente: 1 mensajes
2025-01-30 12:36:23,214 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-01-30 12:36:23,214 - __main__ - INFO - Creando nueva sesión con ID: 13a0d54f-ee2c-42f4-b987-fabbc97de8c2
2025-01-30 12:36:25,169 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:36:28,102 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Starcoder_20250130_095127.md
2025-01-30 12:36:28,102 - __main__ - INFO - Conversación cargada exitosamente: 4 mensajes
2025-01-30 12:36:28,111 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-01-30 12:36:28,112 - __main__ - INFO - Creando nueva sesión con ID: eb908012-42a4-4088-946b-e78e517acd38
2025-01-30 12:36:28,560 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:38:05,734 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/Full_Orca 2_20250130_120806.md
2025-01-30 12:38:05,735 - __main__ - INFO - Conversación cargada exitosamente: 31 mensajes
2025-01-30 12:38:05,740 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2
2025-01-30 12:38:05,740 - __main__ - INFO - Creando nueva sesión con ID: 49d44fe3-67e6-451d-8647-ded3e4e6bfdc
2025-01-30 12:38:06,061 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:47:33,489 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/Full_Orca 2_20250130_120806.md
2025-01-30 12:47:33,490 - __main__ - INFO - Conversación cargada exitosamente: 31 mensajes
2025-01-30 12:47:33,503 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2
2025-01-30 12:47:33,503 - __main__ - INFO - Creando nueva sesión con ID: 8fe4667a-e175-46c4-910a-f16192138494
2025-01-30 12:47:33,960 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 12:47:41,378 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_Orca 2 (Full)_20250130_112055.md
2025-01-30 12:47:41,379 - __main__ - INFO - Conversación cargada exitosamente: 26 mensajes
2025-01-30 12:47:41,388 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2
2025-01-30 12:47:41,388 - __main__ - INFO - Creando nueva sesión con ID: 51aaf05a-0cff-41dc-a88e-c574a6298f83
2025-01-30 12:47:42,658 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 23:55:10,169 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 23:55:10,560 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 23:55:23,460 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-30 23:55:23,623 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-30 23:55:53,321 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 23:55:53,321 - __main__ - INFO - Creando nueva sesión con ID: 7de264cb-1ad5-4d55-ad47-6f778173319f
2025-01-30 23:55:55,646 - __main__ - INFO - Respuesta generada exitosamente
2025-01-30 23:56:05,083 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7de264cb-1ad5-4d55-ad47-6f778173319f, Modelo: Llama 3.1 8B Instruct 128k
2025-01-30 23:56:05,904 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:40:58,188 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 14:40:58,982 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 14:41:36,648 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:41:36,648 - __main__ - INFO - Creando nueva sesión con ID: 5904bf91-4cd5-4385-a919-e714b0dc1c40
2025-01-31 14:41:39,019 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:42:04,532 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:42:08,480 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:43:25,354 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:43:29,352 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:43:51,331 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:43:55,601 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:45:59,131 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:46:03,994 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:47:15,464 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:47:18,417 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:50:21,259 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:50:24,101 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:51:48,105 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:51:52,312 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:55:08,483 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:55:12,048 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 14:58:03,803 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 14:58:08,201 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:03:40,242 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:03:44,883 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:07:58,572 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:08:02,021 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:09:48,543 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:09:51,364 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:12:51,058 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:12:54,767 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:15:58,214 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:16:00,368 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:18:39,784 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:18:42,032 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:19:42,698 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:19:44,750 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:22:35,624 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:22:38,605 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:25:12,208 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:25:15,184 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:27:09,951 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:27:11,857 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:27:18,190 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:27:18,225 - __main__ - ERROR - Error en chat: Error 500: 
2025-01-31 15:27:41,358 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:27:45,525 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:30:22,755 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 15:30:24,340 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 15:30:56,184 - __main__ - INFO - Intentando exportar conversación para sesión: 5904bf91-4cd5-4385-a919-e714b0dc1c40
2025-01-31 15:30:56,184 - __main__ - INFO - Exportando conversación con modelo DeepSeek-R1-Distill-Qwen-14B y servicio gpt4all
2025-01-31 15:30:56,185 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250131_144136.md
2025-01-31 19:54:21,409 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 19:54:21,796 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 19:54:33,499 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 19:54:33,500 - __main__ - INFO - Creando nueva sesión con ID: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d
2025-01-31 19:54:33,618 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 19:55:18,508 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 19:55:24,248 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 19:56:49,365 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 19:56:50,788 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 19:57:10,861 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 19:57:11,884 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 19:59:06,814 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 19:59:11,605 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 20:00:39,989 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 20:00:42,676 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 20:03:08,050 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 20:03:11,790 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 20:05:06,458 - __main__ - INFO - Nueva solicitud de chat - Sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-01-31 20:05:07,955 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 20:06:46,201 - __main__ - INFO - Intentando exportar conversación para sesión: 2663f2c3-2a6f-4d84-b38f-04005fb3b74d
2025-01-31 20:06:46,201 - __main__ - INFO - Exportando conversación con modelo DeepSeek-R1-Distill-Qwen-1.5B y servicio gpt4all
2025-01-31 20:06:46,202 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-1.5B_20250131_195433.md
2025-01-31 23:18:17,749 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 23:18:18,685 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 23:20:01,190 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 23:20:01,430 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 23:20:27,180 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-01-31 23:20:27,180 - __main__ - INFO - Creando nueva sesión con ID: 7826b181-c284-453f-ad5a-588ec9394868
2025-01-31 23:20:28,788 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:21:02,698 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:21:03,613 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:23:12,644 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:23:14,757 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:24:26,407 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:24:29,279 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:25:33,722 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:25:36,594 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:26:33,391 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:26:38,419 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:27:18,636 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:27:32,981 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:28:40,241 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:28:45,355 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:29:47,670 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:29:49,852 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:30:34,816 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:30:38,347 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:31:39,358 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:31:42,247 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:32:52,890 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:32:55,694 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:34:28,632 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:34:30,755 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:35:31,255 - __main__ - INFO - Nueva solicitud de chat - Sesión: 7826b181-c284-453f-ad5a-588ec9394868, Modelo: Phi-3 Mini Instruct
2025-01-31 23:35:35,207 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:38:07,359 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 23:38:07,737 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 23:39:09,298 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-01-31 23:39:09,298 - __main__ - INFO - Creando nueva sesión con ID: 5b207f45-7596-400c-a057-7c8fed103b30
2025-01-31 23:39:12,763 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:39:38,731 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5b207f45-7596-400c-a057-7c8fed103b30, Modelo: Starcoder
2025-01-31 23:39:41,912 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:41:08,669 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5b207f45-7596-400c-a057-7c8fed103b30, Modelo: Starcoder
2025-01-31 23:41:09,881 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:41:59,161 - __main__ - INFO - Nueva solicitud de chat - Sesión: 5b207f45-7596-400c-a057-7c8fed103b30, Modelo: Starcoder
2025-01-31 23:42:03,716 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:43:26,487 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 23:43:26,887 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 23:43:41,217 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:43:41,218 - __main__ - INFO - Creando nueva sesión con ID: bdd5ceae-e168-47b2-b763-81da1b471d27
2025-01-31 23:43:41,315 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:44:14,594 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:44:16,529 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:44:53,948 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:44:59,031 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:45:32,609 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:45:36,773 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:46:16,148 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:46:19,710 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:47:41,219 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:47:44,551 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:49:04,103 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:49:05,567 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:49:46,414 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:49:48,428 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:50:54,336 - __main__ - INFO - Nueva solicitud de chat - Sesión: bdd5ceae-e168-47b2-b763-81da1b471d27, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:50:55,387 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:55:43,779 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-01-31 23:55:44,094 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-01-31 23:56:01,891 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:56:01,892 - __main__ - INFO - Creando nueva sesión con ID: 27af1573-a312-4c86-87d6-43e789fb0979
2025-01-31 23:56:02,006 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:56:24,703 - __main__ - INFO - Nueva solicitud de chat - Sesión: 27af1573-a312-4c86-87d6-43e789fb0979, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:56:28,846 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:56:57,474 - __main__ - INFO - Nueva solicitud de chat - Sesión: 27af1573-a312-4c86-87d6-43e789fb0979, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:57:03,086 - __main__ - INFO - Respuesta generada exitosamente
2025-01-31 23:57:42,405 - __main__ - INFO - Nueva solicitud de chat - Sesión: 27af1573-a312-4c86-87d6-43e789fb0979, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-01-31 23:57:47,717 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:11:31,012 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 00:11:31,254 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 00:12:15,050 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 00:12:15,051 - __main__ - INFO - Creando nueva sesión con ID: 1e5b3e1f-cf62-48f2-a6f9-6124317cf599
2025-02-01 00:12:22,477 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:13:25,590 - __main__ - INFO - Nueva solicitud de chat - Sesión: 1e5b3e1f-cf62-48f2-a6f9-6124317cf599, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 00:13:26,130 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:13:42,237 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 00:13:42,639 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 00:14:17,903 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Orca 2 (Full)
2025-02-01 00:14:17,903 - __main__ - INFO - Creando nueva sesión con ID: ec04176a-e52c-4b17-bd90-c7955feaba56
2025-02-01 00:14:23,145 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:14:42,032 - __main__ - INFO - Nueva solicitud de chat - Sesión: ec04176a-e52c-4b17-bd90-c7955feaba56, Modelo: Orca 2 (Full)
2025-02-01 00:14:48,865 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:15:28,995 - __main__ - INFO - Nueva solicitud de chat - Sesión: ec04176a-e52c-4b17-bd90-c7955feaba56, Modelo: Orca 2 (Full)
2025-02-01 00:15:36,356 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:15:49,793 - __main__ - INFO - Nueva solicitud de chat - Sesión: ec04176a-e52c-4b17-bd90-c7955feaba56, Modelo: Orca 2 (Full)
2025-02-01 00:15:52,055 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 00:16:14,732 - __main__ - INFO - Nueva solicitud de chat - Sesión: ec04176a-e52c-4b17-bd90-c7955feaba56, Modelo: Orca 2 (Full)
2025-02-01 00:16:15,864 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:10:42,529 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:10:42,780 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:11:38,340 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:11:38,341 - __main__ - INFO - Creando nueva sesión con ID: ae55f9f9-ea41-484a-bcaf-7a39d63781c4
2025-02-01 01:12:01,510 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:13:06,462 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:13:06,685 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:13:18,852 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: GPT4All Falcon
2025-02-01 01:13:18,852 - __main__ - INFO - Creando nueva sesión con ID: 7d4cfb72-bfb4-4e1c-aa14-f1fffe60c5ce
2025-02-01 01:13:22,149 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:14:00,693 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:14:00,851 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:14:15,767 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Starcoder
2025-02-01 01:14:15,767 - __main__ - INFO - Creando nueva sesión con ID: b6588c05-4471-4d73-b4c5-86eb0f7d4ebc
2025-02-01 01:14:24,215 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:14:43,216 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:14:43,426 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:14:58,035 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Mistral Instruct
2025-02-01 01:14:58,035 - __main__ - INFO - Creando nueva sesión con ID: 38ae6547-ed7d-45f3-b087-5d186293055f
2025-02-01 01:15:01,500 - __main__ - ERROR - Error en chat: Error 500: 
2025-02-01 01:15:21,773 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Mistral Instruct
2025-02-01 01:15:21,773 - __main__ - INFO - Creando nueva sesión con ID: f8c33a85-bbce-4731-9630-8fafd32ebac4
2025-02-01 01:15:21,780 - __main__ - ERROR - Error en chat: Error 500: 
2025-02-01 01:15:28,774 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:15:29,037 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:16:13,106 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Qwen2-1.5B-Instruct
2025-02-01 01:16:13,106 - __main__ - INFO - Creando nueva sesión con ID: 64f41fcd-1613-49d9-8d81-c3132cd7bd58
2025-02-01 01:16:15,114 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:17:50,656 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:17:50,893 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:20:00,520 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-02-01 01:20:00,520 - __main__ - INFO - Creando nueva sesión con ID: 218a4b86-46e0-418b-ac49-9af7426ddc98
2025-02-01 01:20:05,130 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:20:37,617 - __main__ - INFO - Nueva solicitud de chat - Sesión: 218a4b86-46e0-418b-ac49-9af7426ddc98, Modelo: Phi-3 Mini Instruct
2025-02-01 01:20:41,054 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:23:46,759 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:23:47,070 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:24:10,004 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:24:10,005 - __main__ - INFO - Creando nueva sesión con ID: 525e9129-5fe6-40f6-8f17-e73ab7fe352b
2025-02-01 01:24:15,293 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:24:58,222 - __main__ - INFO - Nueva solicitud de chat - Sesión: 525e9129-5fe6-40f6-8f17-e73ab7fe352b, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:25:06,364 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:26:26,287 - __main__ - INFO - Nueva solicitud de chat - Sesión: 525e9129-5fe6-40f6-8f17-e73ab7fe352b, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:26:49,185 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:30:28,936 - __main__ - INFO - Nueva solicitud de chat - Sesión: 525e9129-5fe6-40f6-8f17-e73ab7fe352b, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:30:41,668 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:31:49,045 - __main__ - INFO - Nueva solicitud de chat - Sesión: 525e9129-5fe6-40f6-8f17-e73ab7fe352b, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:32:15,538 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:35:22,743 - __main__ - INFO - Intentando exportar conversación para sesión: 525e9129-5fe6-40f6-8f17-e73ab7fe352b
2025-02-01 01:35:22,744 - __main__ - INFO - Exportando conversación con modelo DeepSeek-R1-Distill-Qwen-14B y servicio gpt4all
2025-02-01 01:35:22,751 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_012410.md
2025-02-01 01:37:19,005 - __main__ - INFO - Nueva solicitud de chat - Sesión: 525e9129-5fe6-40f6-8f17-e73ab7fe352b, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 01:37:36,650 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 01:58:34,301 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 01:58:35,008 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 01:59:35,475 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Phi-3 Mini Instruct
2025-02-01 01:59:35,475 - __main__ - INFO - Creando nueva sesión con ID: a7c4cd6a-5ab3-447b-a5e6-3bfbc6baf6c3
2025-02-01 01:59:38,715 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:00:30,720 - __main__ - INFO - Nueva solicitud de chat - Sesión: a7c4cd6a-5ab3-447b-a5e6-3bfbc6baf6c3, Modelo: Phi-3 Mini Instruct
2025-02-01 02:00:33,682 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:02:19,768 - __main__ - INFO - Nueva solicitud de chat - Sesión: a7c4cd6a-5ab3-447b-a5e6-3bfbc6baf6c3, Modelo: Phi-3 Mini Instruct
2025-02-01 02:02:22,115 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:03:33,628 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 02:03:33,727 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 02:03:52,055 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:03:52,055 - __main__ - INFO - Creando nueva sesión con ID: fdf303db-7b6e-45ca-84bf-cd963a718c82
2025-02-01 02:03:56,469 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:04:35,732 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:04:39,225 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:06:09,315 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:06:14,753 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:08:23,565 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:08:25,960 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:10:27,053 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:10:31,901 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:12:37,915 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:12:38,993 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:13:39,524 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:13:42,986 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 02:14:27,803 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 02:14:36,580 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 03:02:20,049 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 03:02:29,507 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 03:02:33,779 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 03:02:33,807 - __main__ - ERROR - Error en chat: Error 500: 
2025-02-01 03:02:46,806 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 03:02:49,736 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 03:06:03,278 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 03:06:06,466 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 03:06:43,878 - __main__ - INFO - Nueva solicitud de chat - Sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82, Modelo: Llama 3.1 8B Instruct 128k
2025-02-01 03:06:44,382 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:10:13,629 - __main__ - INFO - Intentando exportar conversación para sesión: fdf303db-7b6e-45ca-84bf-cd963a718c82
2025-02-01 09:10:13,629 - __main__ - INFO - Exportando conversación con modelo Llama 3.1 8B Instruct 128k y servicio gpt4all
2025-02-01 09:10:13,631 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_Llama 3.1 8B Instruct 128k_20250201_020352.md
2025-02-01 09:10:13,645 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_012410.md
2025-02-01 09:10:13,646 - __main__ - INFO - Conversación cargada exitosamente: 10 mensajes
2025-02-01 09:10:13,659 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:10:13,660 - __main__ - INFO - Creando nueva sesión con ID: f01041eb-8166-4567-9e1f-5bd4cb750b93
2025-02-01 09:10:37,232 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:11:02,684 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-1.5B_20250131_195433.md
2025-02-01 09:11:02,684 - __main__ - INFO - Conversación cargada exitosamente: 16 mensajes
2025-02-01 09:11:02,691 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-02-01 09:11:02,691 - __main__ - INFO - Creando nueva sesión con ID: e36f0aab-ccc3-4f00-b698-5d1e42d59f91
2025-02-01 09:11:04,921 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:11:23,661 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250131_144136.md
2025-02-01 09:11:23,662 - __main__ - INFO - Conversación cargada exitosamente: 45 mensajes
2025-02-01 09:11:23,674 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:11:23,674 - __main__ - INFO - Creando nueva sesión con ID: abcb8050-a202-43a2-a0de-383efebb450f
2025-02-01 09:11:42,612 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:11:42,614 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/Full_Orca 2_20250130_120806.md
2025-02-01 09:11:42,615 - __main__ - INFO - Conversación cargada exitosamente: 31 mensajes
2025-02-01 09:11:42,616 - __main__ - INFO - Intentando exportar conversación para sesión: new
2025-02-01 09:11:42,617 - __main__ - ERROR - Sesión no encontrada: new
2025-02-01 09:11:42,618 - __main__ - ERROR - Error al exportar conversación: 404: Sesión no encontrada: new
2025-02-01 09:11:42,627 - __main__ - INFO - Nueva solicitud de chat - Sesión: abcb8050-a202-43a2-a0de-383efebb450f, Modelo: Orca 2
2025-02-01 09:11:49,812 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:11:49,813 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250131_144136.md
2025-02-01 09:11:49,813 - __main__ - INFO - Conversación cargada exitosamente: 45 mensajes
2025-02-01 09:11:49,821 - __main__ - INFO - Nueva solicitud de chat - Sesión: abcb8050-a202-43a2-a0de-383efebb450f, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:12:10,704 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:12:10,705 - __main__ - INFO - Intentando exportar conversación para sesión: abcb8050-a202-43a2-a0de-383efebb450f
2025-02-01 09:12:10,705 - __main__ - INFO - Exportando conversación con modelo DeepSeek-R1-Distill-Qwen-14B y servicio gpt4all
2025-02-01 09:12:10,706 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_091123.md
2025-02-01 09:12:10,717 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_012410.md
2025-02-01 09:12:10,717 - __main__ - INFO - Conversación cargada exitosamente: 10 mensajes
2025-02-01 09:12:10,726 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:12:10,726 - __main__ - INFO - Creando nueva sesión con ID: 1c263a54-7f67-4599-94cc-de6a09c688d5
2025-02-01 09:12:47,345 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:12:47,980 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250131_144136.md
2025-02-01 09:12:47,981 - __main__ - INFO - Conversación cargada exitosamente: 45 mensajes
2025-02-01 09:12:47,998 - __main__ - INFO - Nueva solicitud de chat - Sesión: 1c263a54-7f67-4599-94cc-de6a09c688d5, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:13:12,718 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:13:13,763 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 09:13:14,366 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 09:13:21,250 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_091123.md
2025-02-01 09:13:21,251 - __main__ - INFO - Conversación cargada exitosamente: 3 mensajes
2025-02-01 09:13:21,256 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:13:21,256 - __main__ - INFO - Creando nueva sesión con ID: 6ea5951a-89e2-4c68-a50d-5beb063346b5
2025-02-01 09:13:49,923 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:14:04,320 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 09:14:04,460 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 09:14:10,995 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_012410.md
2025-02-01 09:14:10,995 - __main__ - INFO - Conversación cargada exitosamente: 10 mensajes
2025-02-01 09:14:11,002 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:14:11,002 - __main__ - INFO - Creando nueva sesión con ID: 9c18ae1b-daae-4636-86fe-0cff40290c71
2025-02-01 09:14:38,554 - __main__ - INFO - Respuesta generada exitosamente
2025-02-01 09:14:40,458 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-01 09:14:40,563 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-01 09:15:01,417 - __main__ - INFO - Intentando cargar conversación desde: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250201_012410.md
2025-02-01 09:15:01,418 - __main__ - INFO - Conversación cargada exitosamente: 10 mensajes
2025-02-01 09:15:01,425 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-01 09:15:01,425 - __main__ - INFO - Creando nueva sesión con ID: 3918da34-6062-4055-a540-c2df25516be9
2025-02-01 09:15:29,049 - __main__ - INFO - Respuesta generada exitosamente
2025-02-02 03:05:18,085 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-02 03:05:18,352 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-02 03:06:21,281 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: Llama 3.1 8B Instruct 128k
2025-02-02 03:06:21,281 - __main__ - INFO - Creando nueva sesión con ID: c2fc97a7-b452-49fe-bd2a-5f710337fa6d
2025-02-02 03:06:28,005 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:35:36,978 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-03 10:35:37,544 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-03 10:37:12,562 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-1.5B
2025-02-03 10:37:12,562 - __main__ - INFO - Creando nueva sesión con ID: 1abead48-bbd4-40da-86b0-4bc2e82628db
2025-02-03 10:37:14,217 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:37:28,953 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-03 10:37:29,163 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-03 10:37:42,968 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:37:42,968 - __main__ - INFO - Creando nueva sesión con ID: f01d30c7-a62a-4e32-b351-635f56745df2
2025-02-03 10:37:53,677 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:40:10,132 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:40:22,069 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:42:37,060 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:42:45,611 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:43:17,072 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:43:24,798 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:45:33,206 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:45:36,938 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:47:30,680 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:47:41,187 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:49:43,200 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:49:51,449 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:52:22,784 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:52:38,666 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 10:57:43,815 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 10:58:06,279 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 11:03:22,148 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 11:03:28,433 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 11:04:59,460 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 11:05:12,661 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 11:07:53,791 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 11:08:10,697 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 11:09:19,089 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 11:09:35,937 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 11:10:10,973 - __main__ - INFO - Nueva solicitud de chat - Sesión: f01d30c7-a62a-4e32-b351-635f56745df2, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 11:10:13,798 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 12:44:05,642 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-03 12:44:06,138 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-03 14:44:25,071 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 14:44:25,071 - __main__ - INFO - Creando nueva sesión con ID: acca92f5-1a1d-436f-8e01-e978916d8a67
2025-02-03 14:44:35,515 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 14:50:14,166 - __main__ - INFO - Nueva solicitud de chat - Sesión: acca92f5-1a1d-436f-8e01-e978916d8a67, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-03 14:50:28,710 - __main__ - INFO - Respuesta generada exitosamente
2025-02-03 17:10:44,416 - __main__ - INFO - Intentando exportar conversación para sesión: acca92f5-1a1d-436f-8e01-e978916d8a67
2025-02-03 17:10:44,417 - __main__ - INFO - Exportando conversación con modelo DeepSeek-R1-Distill-Qwen-14B y servicio gpt4all
2025-02-03 17:10:44,424 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250203_144425.md
2025-02-04 20:27:47,814 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-04 20:27:48,099 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-04 20:32:22,145 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-04 20:32:22,622 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-05 12:30:17,256 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-05 12:30:17,566 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
2025-02-05 12:30:42,796 - __main__ - INFO - Nueva solicitud de chat - Sesión: new, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 12:30:42,796 - __main__ - INFO - Creando nueva sesión con ID: 30b58945-a7c2-438b-aa91-0724385761a6
2025-02-05 12:31:05,483 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 12:31:34,479 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 12:32:03,850 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 12:53:31,122 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 12:53:47,922 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 12:55:15,870 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 12:55:29,817 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 12:57:11,878 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 12:57:34,537 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 13:01:28,361 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 13:01:49,814 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 13:02:53,150 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 13:03:07,850 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 13:07:30,494 - __main__ - INFO - Nueva solicitud de chat - Sesión: 30b58945-a7c2-438b-aa91-0724385761a6, Modelo: DeepSeek-R1-Distill-Qwen-14B
2025-02-05 13:07:35,354 - __main__ - INFO - Respuesta generada exitosamente
2025-02-05 13:09:38,421 - __main__ - INFO - Intentando exportar conversación para sesión: 30b58945-a7c2-438b-aa91-0724385761a6
2025-02-05 13:09:38,421 - __main__ - INFO - Exportando conversación con modelo DeepSeek-R1-Distill-Qwen-14B y servicio gpt4all
2025-02-05 13:09:38,428 - __main__ - INFO - Conversación exportada exitosamente a: /home/jose/GPT_Local/chat_history/gpt4all_DeepSeek-R1-Distill-Qwen-14B_20250205_123042.md
2025-02-06 10:59:41,222 - __main__ - INFO - Obteniendo lista de modelos disponibles...
2025-02-06 10:59:41,852 - __main__ - INFO - Modelos encontrados: [{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': "{{- '<|im_start|>system\\n' }}\n{% if toolList|length > 0 %}You have access to the following functions:\n{% for tool in toolList %}\nUse the function '{{tool.function}}' to: '{{tool.description}}'\n{% if tool.parameters|length > 0 %}\nparameters:\n{% for info in tool.parameters %}\n  {{info.name}}:\n    type: {{info.type}}\n    description: {{info.description}}\n    required: {{info.required}}\n{% endfor %}\n{% endif %}\n# Tool Instructions\nIf you CHOOSE to call this function ONLY reply with the following format:\n'{{tool.symbolicFormat}}'\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\n'{{tool.exampleCall}}'\nAfter the result you might reply with, '{{tool.exampleReply}}'\n{% endfor %}\nYou MUST include both the start and end tags when you use a function.\n\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\n{% endif %}\n{{- '<|im_end|>\\n' }}\n{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n{{ '<|im_start|>assistant\\n' }}\n{% endif %}\n", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3/license/">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {{- content }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': "{%- if not add_generation_prompt is defined %}\n    {%- set add_generation_prompt = false %}\n{%- endif %}\n{%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<｜User｜>' + message['content'] }}\n    {%- endif %}\n    {%- if message['role'] == 'assistant' %}\n        {%- set content = message['content'] | regex_replace('^[\\\\s\\\\S]*</think>', '') %}\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\n    {%- endif %}\n{%- endfor -%}\n{%- if add_generation_prompt %}\n    {{- '<｜Assistant｜>' }}\n{%- endif %}"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_2/license/">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{{- bos_token }}\n{%- set date_string = strftime_now('%d %b %Y') %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] | trim %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set system_message = '' %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n\n{#- System message #}\n{{- '<|start_header_id|>system<|end_header_id|>\\n\\n' }}\n{{- 'Cutting Knowledge Date: December 2023\\n' }}\n{{- 'Today Date: ' + date_string + '\\n\\n' }}\n{{- system_message }}\n{{- '<|eot_id|>' }}\n\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_start = 1 %}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\n                {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}\n            {%- else %}\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\n            {%- endif %}\n        {%- elif message['role'] == 'assistant' %}\n            {{- ' ' + message['content'] + eos_token }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}"}, {'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href="https://llama.meta.com/llama3_1/license/">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\n\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\n\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': "{%- set loop_messages = messages %}\n{%- for message in loop_messages %}\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\n    {%- if loop.index0 == 0 %}\n        {%- set content = bos_token + content %}\n    {%- endif %}\n    {{- content }}\n{%- endfor %}\n{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href="https://atlas.nomic.ai/">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\n<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User: ' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Assistant: ' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Assistant:' }}\n{%- endif %}"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + ' ' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\n            {{- ' ' }}\n        {%- else %}\n            {{- eos_token }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\n%1</s>\n<|assistant|>\n%2</s>\n', 'systemPrompt': '<|system|>\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\n</s>', 'chatTemplate': "{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '<|user|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<|system|>\\n' + message['content'] + eos_token }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '<|assistant|>\\n'  + message['content'] + eos_token }}\n    {%- endif %}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|assistant|>' }}\n    {%- endif %}\n{%- endfor %}", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\n%1\n\n### Response:\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Instruction:\\n' }}\n{%- endif %}"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### Instruction:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>\n', 'systemPrompt': '<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href="https://opensource.org/license/mit">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\n%1<|end|>\n<|assistant|>\n%2<|end|>\n', 'systemPrompt': '', 'chatTemplate': "{{- bos_token }}\n{%- for message in messages %}\n    {{- '<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\\n' }}\n{%- else %}\n    {{- eos_token }}\n{%- endif %}"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\n%1\n\n### Response:\n', 'systemPrompt': '### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- '### System:\\n' + messages[0]['content'] + '\\n\\n' }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if message['role'] == 'user' %}\n            {{- '### User:\\n' + message['content'] + '\\n\\n' }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- '### Response:\\n' + message['content'] + '\\n\\n' }}\n        {%- else %}\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '### Response:\\n' }}\n{%- endif %}"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': "{%- if messages[0]['role'] == 'system' %}\n    {%- set loop_start = 1 %}\n    {{- messages[0]['content'] }}\n{%- else %}\n    {%- set loop_start = 0 %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if loop.index0 >= loop_start %}\n        {%- if not loop.first %}\n            {{- ' ' }}\n        {%- endif %}\n        {%- if message['role'] == 'user' %}\n            {{- 'USER: ' + message['content'] }}\n        {%- elif message['role'] == 'assistant' %}\n            {{- 'ASSISTANT: ' + message['content'] }}\n        {%- else %}\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {%- if messages %}\n        {{- ' ' }}\n    {%- endif %}\n    {{- 'ASSISTANT:' }}\n{%- endif %}", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}, {'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href="https://www.apache.org/licenses/LICENSE-2.0.html/">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\n%1<|im_end|>\n<|im_start|>assistant\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n', 'chatTemplate': "{%- for message in messages %}\n    {%- if loop.first and messages[0]['role'] != 'system' %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}"}]
